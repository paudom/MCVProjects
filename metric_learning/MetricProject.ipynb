{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MetricProject.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPfqgKRNrKUBNTfZZklhUc9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"DJwdY9JAHQ0H","colab_type":"text"},"source":["# METRIC LEARNING\n","\n","\n","1.   Configuration\n","2.   Important Repositories\n","3.   Preparing and Loading Data\n","4.   Training Embedder and Classifier\n","5.   Observing embeddings (T-SNE, UMAP)\n","6.   Simaese vs Triplets\n","\n"]},{"cell_type":"markdown","metadata":{"id":"vmIKaY8IHvc7","colab_type":"text"},"source":["# 1. Configuration"]},{"cell_type":"code","metadata":{"id":"WLANIYtt-7Fc","colab_type":"code","outputId":"f9d1a1f4-e390-420b-ec1c-c862147ab21c","executionInfo":{"status":"ok","timestamp":1587402270640,"user_tz":-120,"elapsed":1233,"user":{"displayName":"Pablo Domingo Gregorio","photoUrl":"","userId":"14570453397954729021"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"aPQ3yEJ9Asc1","colab_type":"code","outputId":"85c73510-32df-4d45-d521-c230ce99d178","executionInfo":{"status":"ok","timestamp":1587398319720,"user_tz":-120,"elapsed":3560,"user":{"displayName":"Pablo Domingo Gregorio","photoUrl":"","userId":"14570453397954729021"}},"colab":{"base_uri":"https://localhost:8080/","height":84}},"source":["%cd \"/content/drive/My Drive/MCV/Exercise 1\"\n","!ls"],"execution_count":7,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/MCV/Exercise 1\n","Data  losses.py\t\t   metrics.py  networks.py\t   trainer.py\n","dddd  MetricProject.ipynb  mmfashion   prepare_in_shop.py  utils.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LyxpmEG1UZ8K","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":84},"outputId":"223933fc-92dd-4d53-bf34-dc65d126cc51","executionInfo":{"status":"ok","timestamp":1587398488131,"user_tz":-120,"elapsed":6157,"user":{"displayName":"Pablo Domingo Gregorio","photoUrl":"","userId":"14570453397954729021"}}},"source":["# -- INSTALL MMFASHION DEPENDENCIES -- #\n","%cd \"mmfashion\"\n","!python setup.py install\n","!ls"],"execution_count":9,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/MCV/Exercise 1/mmfashion\n","python3: can't open file 'setup.py': [Errno 2] No such file or directory\n","apis  core  datasets  __init__.py  models  __pycache__\tutils  version.py\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5-dCcCacGud7","colab_type":"text"},"source":["# 2. Important Repositories\n","\n","1.   Siamese and Triplet Networks [repo](https://github.com/adambielski/siamese-triplet)\n","2.   MMFashion Datasets [repo](https://github.com/open-mmlab/mmfashion/)\n","\n"]},{"cell_type":"code","metadata":{"id":"8L1fwjeTRFkX","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":567},"outputId":"66263115-c7a2-4d28-ac5d-70660ecb1d0d","executionInfo":{"status":"error","timestamp":1587398347996,"user_tz":-120,"elapsed":7649,"user":{"displayName":"Pablo Domingo Gregorio","photoUrl":"","userId":"14570453397954729021"}}},"source":["# -- IMPORTS -- #\n","import torch\n","import numpy as np\n","import os\n","import torch.optim as optim\n","from torch.optim import lr_scheduler\n","from torch.autograd import Variable\n","from networks import EmbeddingNet, ClassificationNet\n","from metrics import AccumulatedAccuracyMetric\n","from trainer import fit"],"execution_count":8,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-1b4ca8bba96e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmmfashion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInShopDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmmfashion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbuild_dataloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/My Drive/MCV/Exercise 1/mmfashion/datasets/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mAttr_Pred\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAttrDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbuild_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mConsumer_to_shop\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConsumerToShopDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdataset_wrappers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConcatDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mIn_shop\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInShopDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/My Drive/MCV/Exercise 1/mmfashion/datasets/Attr_Pred.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mregistry\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDATASETS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/My Drive/MCV/Exercise 1/mmfashion/datasets/registry.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRegistry\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mDATASETS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRegistry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dataset'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/My Drive/MCV/Exercise 1/mmfashion/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_img_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mregistry\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRegistry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_from_cfg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcheckpoint\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minit_weights_from\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m __all__ = ['Registry', 'build_from_cfg', 'get_img_tensor',\n","\u001b[0;32m/content/drive/My Drive/MCV/Exercise 1/mmfashion/utils/registry.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmmcv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mmcv'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}]},{"cell_type":"markdown","metadata":{"id":"z_-6qwfxQHm2","colab_type":"text"},"source":["# 3. Preparing and Loading Data\n","\n","Here we will use the mmfashion library to load the data and be ready to load them normally or if we want woth anchor, positive and negative samples.\n","\n"]},{"cell_type":"code","metadata":{"id":"-uqXsd_TQ5bA","colab_type":"code","colab":{}},"source":["# -- CONSTANTS -- #\n","img_path = os.path.join(working_dir, 'Data')\n","annotations_path = os.path.join(img_path, 'Anno')\n","batch_size = 256\n","class_categories = {\n","    'Denim': 0,\n","    'Jackets_Vests': 1,\n","    'Pants': 2,\n","    'Shirts_Polos': 3,\n","    'Shorts': 4,\n","    'Suiting': 5,\n","    'Sweaters': 6,\n","    'Sweatshirts_Hoodies': 7,\n","    'Tees_Tanks': 8,\n","    'Blouses_Shirts': 9,\n","    'Cardigans': 10,\n","    'Dresses': 11,\n","    'Graphic_Tees': 12,\n","    'Jackets_Coats': 13,\n","    'Leggings': 14,\n","    'Rompers_Jumpsuits': 15,\n","    'Skirts': 16\n","}\n","num_classes = 17"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qnCjWld8RABn","colab_type":"code","colab":{}},"source":["# -- DATASETS AND DATALOADERS -- #\n","train_dataset = InShopDataset(img_path=img_path,\n","                              img_file=os.path.join(annotations_path,'train_img.txt'),\n","                              label_file=os.path.join(annotations_path,'train_labels.txt'),\n","                              id_file=os.path.join(annotations_path,'train_id.txt'),\n","                              bbox_file=os.path.join(annotations_path,'train_bbox.txt'),\n","                              landmark_file=os.path.join(annotations_path,'train_landmarks.txt'),\n","                              img_size=[256, 256],\n","                              class_mapping=class_mapping)\n","\n","query_dataset = InShopDataset(img_path=img_path,\n","                              img_file=os.path.join(annotations_path,'query_img.txt'),\n","                              label_file=os.path.join(annotations_path,'query_labels.txt'),\n","                              id_file=os.path.join(annotations_path,'query_id.txt'),\n","                              bbox_file=os.path.join(annotations_path,'query_bbox.txt'),\n","                              landmark_file=os.path.join(annotations_path,'query_landmarks.txt'),\n","                              img_size=[256, 256],\n","                              class_mapping=class_mapping)\n","\n","cuda = torch.cuda.is_available()\n","\n","train_dataloader = build_dataloader(dataset=train_dataset,\n","                                           imgs_per_gpu=batch_size,\n","                                           workers_per_gpu=1,\n","                                           num_gpus=1,\n","                                           shuffle=True)\n","\n","query_dataloader = build_dataloader(dataset=query_dataset,\n","                                           imgs_per_gpu=batch_size,\n","                                           workers_per_gpu=1,\n","                                           num_gpus=1,\n","                                           shuffle=False)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vH7i0n5JTCwz","colab_type":"text"},"source":["# 4. Training Embedder and Classifier\n","\n","In this section we will train and embedder and a classifier. This way, we will obtain and embedding which will be plotted later. \n","\n","The embedder is a CNN that uses:\n","*   Elemento de lista\n","*   Elemento de lista\n","\n","The classifier is just a NLP with one layer that transforms the embedder output to a dense-softmax layer to make the classification task"]},{"cell_type":"code","metadata":{"id":"3TnBU68zSqHF","colab_type":"code","colab":{}},"source":["# -- CONFIGURE EMBEDDER AND CLASSIFIER HEAD -- #\n","embedding_net = EmbeddingNet()\n","model = ClassificationNet(embedding_net, n_classes=n_classes)\n","if cuda:\n","    model.cuda()\n","loss_fn = torch.nn.NLLLoss()\n","lr = 1e-2\n","optimizer = optim.Adam(model.parameters(), lr=lr)\n","scheduler = lr_scheduler.StepLR(optimizer, 8, gamma=0.1, last_epoch=-1)\n","n_epochs = 20\n","log_interval = 50"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"I1VbOrL1Wjcl","colab_type":"code","colab":{}},"source":["# -- TRAIN EMBEDDER AND CLASSIFIER -- #\n","fit(train_dataloader,\n","    query_dataloader,\n","    model,\n","    loss_fn,\n","    optimizer,\n","    scheduler,\n","    n_epochs,\n","    cuda,\n","    log_interval,\n","    metrics=[AccumulatedAccuracyMetric()]\n",")"],"execution_count":0,"outputs":[]}]}